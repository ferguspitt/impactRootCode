{
 "metadata": {
  "name": "NewsCitationsOnWikipedia"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "header1Type='From'\nheader1Value='ferguspitt@columbia.edu'\ndataLocation='/Users/ferguspitt/impactRootCode/data/'\noutputLocation='/Users/ferguspitt/impactRootCode/outputs/'\nlogsLocation='/Users/ferguspitt/impactRootCode/logs/'\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "%pylab inline\nimport urllib2 \nimport html5lib\nimport json\nimport re\nfrom bs4 import BeautifulSoup \nimport time\nfrom urlparse import urlparse\nimport operator\nfrom datetime import datetime\nimport csv\nimport matplotlib.pyplot as plt",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\n\ndef getPage(compiledURL,headerType,headerValue):\n    ''' This function calls a URL. It relies on a number of global variables being set: header1Type, header1Value, filesLocation'''\n    request = urllib2.Request(compiledURL)\n    request.add_header(headerType,headerValue)\n    i=0\n    while i<4:\n        try: \n            response = urllib2.urlopen(request)\n            headers = response.info()\n            data = response.read()\n            i=4\n        except urllib2.HTTPError, e:\n            fh=open(logsLocation+'RequestFailures.log','a')\n            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'HTTPError = ' + str(e.code))\n            fh.close()\n            i=i+1\n            time.sleep(2)\n            data=''\n            print 'HTTPError'\n        except urllib2.URLError, e:\n            fh=open(logsLocation+'RequestFailures.log','a')\n            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'URLError = ' + str(e.reason))\n            fh.close()\n            i=i+1\n            time.sleep(2)\n            data=''\n            print 'URLError'\n        except httplib.HTTPException, e:\n            fh=open(logsLocation+'RequestFailures.log','a')\n            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'HTTPException')\n            fh.close()\n            i=i+1\n            time.sleep(2)\n            data=''\n            print 'HTTPException'\n        except Exception:\n            import traceback\n            fh=open(logsLocation+'RequestFailures.log','a')\n            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'generic exception: ' + traceback.format_exc())\n            fh.close()\n            i=i+1\n            time.sleep(2)\n            data=''\n            print 'exception'\n    print \"called: \"+ compiledURL;\n    return data\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def rollUp(endDomain,startDomain,startURL):\n    rollLog=open(logsLocation+endDomain+'RollUpLog.log','a')\n    rollLog.write(str(startURL)+':\\n'+startDomain+'\\n\\n')\n    rollLog.close()\n    return endDomain\n\ndef extractFootNotes(topicURL):\n    topicData=BeautifulSoup(getPage(topicURL,header1Type,header1Value),'html5lib')\n    \n    Footnotes=[]\n    citations=topicData.findAll('span',{'class': re.compile('^citation')})\n    # check for common (annoying) archiving sites:\n    #print citations\n    for citation in citations:\n        if len(citation.findAll('a',{'class': re.compile('^external')})) ==1:\n            Footnotes.append(citation.findAll('a',{'class':re.compile('^external')})[0])\n        else:\n            if len(citation.findAll('a',{'class': re.compile('^external')})) >1:\n                for link in citation.findAll('a',{'class': re.compile('^external')}):\n                    archiveService=['webcitation.org','web.archive.org','archive.is','dx.doi.org']\n                    href=str(link.get('href'))\n                    if any(x in href for x in archiveService):\n                        webcitationLog=open(logsLocation+'badcitationLog.log','a')\n                        problemcite=str(link.get('href'))+' triggered this citation condition from:'+str(citation)\n                        webcitationLog.write(topicURL+'\\n')\n                        webcitationLog.write(problemcite)\n                        webcitationLog.write('\\n\\n')\n                        webcitationLog.close()\n                       # print 'citation used an archive service'\n                    else:\n                        #print 'more than two links, we\\'re choosing: '+str(link)\n                        leftoverLog=open(logsLocation+'SelectionFromArchive.log','a')\n                        problemcite='Citation has more than two links, we\\'re including: '+str(link.get('href'))+'\\n from:'+str(citation)\n                        leftoverLog.write(topicURL+'\\n')\n                        leftoverLog.write(problemcite)\n                        leftoverLog.write('\\n\\n')\n                        leftoverLog.close()\n                        Footnotes.append(link)\n    #print Footnotes\n    #OK, we should have clean footnotes, now sort them into domains, count them etc.\n    URLDict={'ExternalURLs':[]}\n\n    for footnote in Footnotes:\n        URL=re.findall(r'href=[\\'\"]?([^\\'\" >]+)',str(footnote))\n        if URL[0].startswith(\"http:\") == True:\n            URLDict['ExternalURLs'].append(URL[0])\n    #make it easier to read\n    URLDict['ExternalURLs'].sort()\n    #count the number of ExternalURLS\n    URLLog=open(logsLocation+'URLLog.log','a')\n    URLLog.write('\\n\\n\\n\\n'+topicURL+' at:')\n    timestamp = str(datetime.now())\n    URLLog.write(timestamp+'\\n\\n')\n    URLDict['NumberOfURLs']=len(URLDict['ExternalURLs'])\n    # get the unique domains.\n    uniqueDomains={}\n    checkDomainCount=0\n    #print URLDict['ExternalURLs']\n    for URL in URLDict['ExternalURLs']:\n        URLLog.write(URL+'\\n')\n        domain =''\n        #checkeddomain =''\n        parsed_uri = urlparse(URL)\n        domain = '{uri.netloc}'.format(uri=parsed_uri).replace('www.','') #hey look, we're getting rid of the www. This may not be smart.\n        #print 'Raw Domain:' + domain\n        if domain=='':\n            print 'top: '+URL     \n        elif domain[-12:] =='.abcnews.com' or re.search('abc(\\w*).go.com',domain):\n            domain=rollUp('abcnews.com',domain,URL)\n            #print domain\n        elif 'http://www.google.com/hostednews/afp/' in str(URL):\n            domain=rollUp('afp.com',domain,URL)\n            #print domain\n        elif domain in ['aljazeera.net'] or domain[-14:] =='.aljazeera.com':\n            domain=rollUp('aljazeera.com',domain,URL)\n            #print domain\n        elif 'http://www.google.com/hostednews/ap/' in str(URL) or domain[-7:]=='.ap.org': #this captures some 'hosted' URLs\n            domain=rollUp('ap.org',domain,URL)\n        elif domain[-17:] =='.baltimoresun.com':\n            domain=rollUp('baltimoresun.com',domain,URL)\n        elif domain in ['bbc.co.uk','news.bbc.co.uk','bbcnews.co.uk','bbcnews.com']:\n            domain=rollUp('bbc.co.uk',domain,URL)\n        elif domain in ['boston.com'] or domain[-16:] =='.bostonglobe.com':\n            domain=rollUp('bostonglobe.com',domain,URL)\n        elif domain[-16:] == 'businessweek.com':\n            domain=rollUp('bloomberg.com',domain,URL)\n        elif domain[-13:] =='.cbslocal.com': \n            domain=rollUp('cbsnews.com',domain,URL)\n        elif domain[-8:]=='.cnn.com': \n            domain=rollUp('cnn.com',domain,URL)\n        elif domain[-11:]=='.forbes.com':\n            domain=rollUp('forbes.com',domain,URL)\n        elif domain[-7:]=='.ft.com':\n            domain=rollUp('ft.com',domain,URL)\n        elif domain in ['guardian.co.uk','theguardian.co.uk','theguardiannews.com','guardiannews.com','theguardian.com','m.guardiannews.com']:#deliberately verbose: there are copycat guardian brands.\n            domain=rollUp('theguardian.com',domain,URL)\n        elif domain[-12:] == '.latimes.com':\n            domain=rollUp('latimes.com',domain,URL)\n        elif domain in ['msnbc.msn.com','msnbcmedia.msn.com'] or domain[-12:] == '.nbcnews.com' or re.search('nbc(\\w*).com',domain): #too greedy?\n            domain=rollUp('nbcnews.com',domain,URL)\n        elif domain[-12:]== '.nytimes.com':\n            domain=rollUp('nytimes.com',domain,URL)\n        elif domain[-8:] == '.npr.org': \n            domain=rollUp('npr.org',domain,URL)\n        elif domain[-12:]=='.reuters.com':\n            domain=rollUp('reuters.com',domain,URL)\n        elif domain[-20:]=='.orlandosentinel.com':\n            domain=rollUp('orlandosentinel.com',domain,URL)\n        elif domain[-16:]=='.telegraph.co.uk':\n            domain=rollUp('telegraph.co.uk',domain,URL)\n        elif domain[-13:]=='.usatoday.com':\n            domain=rollUp('usatoday.com',domain,URL)\n        elif domain[-15:]=='.news.yahoo.com':\n            domain=rollUp('news.yahoo.com',domain,URL)\n        elif domain[-19:]=='.washingtonpost.com':\n            domain=rollUp('washingtonpost.com',domain,URL)\n        elif domain[-8:]=='.wsj.com':\n            domain=rollUp('wsj.com',domain,URL)\n        \n        #print 'passing domain to checkeddomain'\n        checkeddomain=domain\n            \n        if checkeddomain in uniqueDomains.keys():\n            uniqueDomains[checkeddomain]=uniqueDomains[checkeddomain]+1\n            URLLog.write('resolves to: '+checkeddomain+'\\n')\n        else:\n            \n            uniqueDomains[checkeddomain]=1\n            URLLog.write('resolves to: '+checkeddomain+'\\n')\n    URLLog.close()\n    URLDict['uniqueDomains']=uniqueDomains\n    URLDict['NumberofUniqueDomains']=len(URLDict['uniqueDomains'])\n    for Udomain in uniqueDomains: \n        checkDomainCount=uniqueDomains[Udomain]+checkDomainCount\n    URLDict['CheckCount']=checkDomainCount\n    if URLDict['CheckCount']!=URLDict['NumberOfURLs']:\n        print 'Dropped Footnotes!: URLs Sorted By Domain=' +str(URLDict['CheckCount'])+' but Number of URLs (Raw)= '+str(URLDict['NumberOfURLs'])\n    return URLDict['uniqueDomains']\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Given a Wikipedia URL, we can now get the footnotes in a dictionary\ndef runIt(Stories):    \n    topStoryFootnotes=[]\n    for story in Stories['Stories']:\n        #print '\\n'+story.keys()[0] #debug\n        print '.'\n        key=story.keys()[0]\n        keyDict={key:{}}\n        for wikipage in story[key]:\n            #topStoryFootnotes[story]={'results'={key=wikipage}}\n            #results = extractFootNotes('http://en.wikipedia.org/wiki/'+wikipage) #maybe change this to simply take and pass the full wikipedia URL\n            results = extractFootNotes(wikipage) \n            keyDict[key][wikipage]=results\n        topStoryFootnotes.append(keyDict)\n    return topStoryFootnotes\n\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# the function that counts the number of citations per domain.\ndef countOverAllDomains(dataSource):\n    topDomains={}\n    topDomainsByStory={}\n    storyTopDomainsSorted={}\n    for story in dataSource:\n        topDomainsByStory[story.keys()[0]]={}\n        for wikipage in story:\n            for domainList in story[wikipage].keys():\n                for domain in story[wikipage][domainList].items():\n                    if domain[0] not in topDomains.keys():\n                        topDomains[domain[0]]=domain[1]\n                        \n                    else:\n                        topDomains[domain[0]]=topDomains[domain[0]]+domain[1]\n                    #now make a dictionary for each story\n                    if domain[0] not in topDomainsByStory[story.keys()[0]].keys():\n                        topDomainsByStory[story.keys()[0]][domain[0]]=domain[1]\n                    else:\n                        topDomainsByStory[story.keys()[0]][domain[0]]=topDomainsByStory[story.keys()[0]][domain[0]]+domain[1]\n        storyTopDomainsSorted[story.keys()[0]]=sorted(topDomainsByStory[story.keys()[0]].items(), key=operator.itemgetter(1), reverse=True)\n    \n    sorted_Domains = sorted(topDomains.items(), key=operator.itemgetter(1), reverse=True) \n\n    return sorted_Domains, storyTopDomainsSorted\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def makePie(CSVfile,V,Title):\n\n\n    source=[]\n    citations=[]\n    wedgeN=[]\n    wedgeL=[]\n    clrs=['#003333','#004747','#005B5B','#007070','#008484','#009999','#00ADAD','#00C1C1','#00D6D6','#00EAEA','#00EFFF']\n    i=0\n\n\n    with open(CSVfile,'r') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            source.append(row[0]+\": (\"+row[1]+\")\")\n            citations.append(row[1])\n    longtail=0\n    for n in citations[11:]:\n        longtail=longtail+int(n)\n    str(longtail)\n    wedges=citations[1:11]\n    wedges.append(str(longtail))\n    labels=source[1:11]\n    labels.append('others: ('+str(longtail)+\")\")\n    figure(V, figsize=(10,10))\n    plt.pie(wedges,labels=labels,colors=clrs,startangle=90)\n    title(Title, fontsize='22')\n    savefig(outputLocation+Title+'.svg', bbox_inches='tight')\n    savefig(outputLocation+Title+'.png', bbox_inches='tight')\n    plt.close()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#this function goes to the current events portal, and gets wikipedia URLs for only the currently listed topics.\ndef fetchTopicsInTheNews():\n    wikiNewsData=getPage('https://en.wikipedia.org/wiki/Portal:Current_events',header1Type,header1Value)\n    currentEventsHTML=BeautifulSoup(wikiNewsData,'html5lib')\n    currentEventsTable=''\n    pageLinks=[]\n    tables = currentEventsHTML.findAll('table')\n    for table in tables:\n        rows=table.findAll('tr')\n        for row in rows:\n            if 'Topics in the news' in str(row):\n                #print 'found the news table'\n                currentEventsTable=table\n    for link in currentEventsTable.findAll('a'):\n        pageLinks.append('https://en.wikipedia.org'+str(link.get('href')))\n\n    return pageLinks",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#\ndef byteify(input):\n        if isinstance(input, dict):\n            return {byteify(key):byteify(value) for key,value in input.iteritems()}\n        elif isinstance(input, list):\n            return [byteify(element) for element in input]\n        elif isinstance(input, unicode):\n            return input.encode('utf-8')\n        else:\n            return input\n\ndef loadData(filename):\n    with open(dataLocation+filename,'r') as data_file: \n        #storyList=data_file.read()\n        \n        storyjson = json.load(data_file)\n\n    return byteify(storyjson)\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def getTimeLabel():\n    tt=str(time.asctime( time.localtime(time.time()) ))\n    return tt[0:10]+\" \"+tt[-4:]\n\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#lclStoryDict={}\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def wikiNewsToJson(jsonFile):\n    try:\n        lclStoryDict=loadData(jsonFile)\n        #see whether it has our date\n        for item in lclStoryDict['Stories']:\n            if getTimeLabel() not in item.keys():\n                #print \"we don't have the key\"\n                lclStoryDict['Stories'].append({getTimeLabel():fetchTopicsInTheNews()})\n                with open(dataLocation+jsonFile, 'w+') as fp:\n                    json.dump(lclStoryDict,fp)\n            else:\n                print \"have the date\" #at the moment this structure wastes cycles, because even after it's found the date it keeps cylcing.\n    except:\n        print 'some kind of failure above'\n        with open(dataLocation+jsonFile, 'w+') as fp: #so create/open a file at that location\n            json.dump({'Stories':[{getTimeLabel():fetchTopicsInTheNews()}]}, fp)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def writeOutPuts(inputDict):\n    #this processes the data and creates csv and image files. It expects a dictionary in a particular form.\n    rank=countOverAllDomains(inputDict)\n\n    #write out a Master List\n    with open(outputLocation+'compiledResults.csv','w') as csvfile:\n        outputWriter=csv.writer(csvfile)\n        lst=['Domain','Citations']\n        outputWriter.writerow(lst) \n        for domain in rank[0]:\n            outputWriter.writerow(domain)\n    csvfile.close\n    makePie(outputLocation+'compiledResults.csv',1,'Compiled Results')\n    print 'master csv and charts saved'\n\n    #write out files for each story\n    i=2\n    for story in rank[1].keys():\n        \n        with open(outputLocation+story+'Results.csv','w') as csvSubFile:\n            outputWriter=csv.writer(csvSubFile)\n            lst=['Domain','Citations']\n            outputWriter.writerow(lst)  \n            for domain in rank[1][story]:            \n                outputWriter.writerow(domain)\n        csvfile.close\n        makePie(outputLocation+story+'Results.csv',i,story)\n        i=i+1\n        print 'csv and charts saved for '+story",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "wikiNewsToJson('runningDates.json')\nstoryDict=loadData('runningDates.json')\n#CurrentTopicsDict=loadData('June23Stories.json')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "smallData=runIt(storyDict)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "writeOutPuts(smallData)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}