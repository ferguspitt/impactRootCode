{
 "metadata": {
  "name": "NewsCitationsOnWikipedia"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header1Type='From'\n",
      "header1Value='yourname@domain.tld'\n",
      "outputLocation='../outputs/'\n",
      "logsLocation='../logs/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2 \n",
      "import html5lib\n",
      "import re\n",
      "from bs4 import BeautifulSoup \n",
      "import time\n",
      "from urlparse import urlparse\n",
      "import operator\n",
      "from datetime import datetime\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#some data to munch on. This is for 2013\n",
      "\n",
      "storyList=[{'Syrian Civil War': {'wikipages': ['Syrian_civil_war',\n",
      "    'Timeline_of_the_Syrian_civil_war_(January-April_2013)',\n",
      "    'Casualties_of_the_Syrian_Civil_War']}},\n",
      " {'Obamacare': {'wikipages': ['Patient_Protection_and_Affordable_Care_Act',\n",
      "    'Affordable_Health_Care_for_America_Act',\n",
      "    'Constitutional_challenges_to_the_Patient_Protection_and_Affordable_Care_Act']}},\n",
      " {'US Government Shutdown': {'wikipages': ['United_States_federal_government_shutdown_of_2013',\n",
      "    'Government_shutdown_in_the_United_States',\n",
      "    'United_States_debt-ceiling_crisis_of_2013']}},\n",
      " {'Edward Snowden': {'wikipages': ['Global_surveillance_disclosure',\n",
      "    'Edward_Snowden',\n",
      "    'PRISM_(surveillance_program)']}},\n",
      " {'Boston Marathon Bombing': {'wikipages': ['Boston_Marathon_bombings',\n",
      "    '2013_Boston_Marathon',\n",
      "    'Dzhokhar_and_Tamerlan_Tsarnaev']}},\n",
      " {'Nelson Mandela Death': {'wikipages': ['Death_and_state_funeral_of_Nelson_Mandela',\n",
      "    'Nelson_Mandela',\n",
      "    'Makgatho_Mandela']}},\n",
      " {'Pope Francis': {'wikipages': ['Pope_Francis',\n",
      "    'Theology_of_Pope_Francis',\n",
      "    'Coat_of_arms_of_Pope_Francis']}},\n",
      " {'George Zimmerman Trial': {'wikipages': ['State_of_Florida_v._George_Zimmerman',\n",
      "    'George_Zimmerman',\n",
      "    'Shooting_of_Trayvon_Martin']}},\n",
      " {'US Economy': {'wikipages': ['Economy_of_the_United_States',\n",
      "    'Economy_of_the_United_States_by_sector',\n",
      "    'Economic_history_of_the_United_States']}},\n",
      " {'Egypt Coup': {'wikipages': ['Anti-Coup_Alliance',\n",
      "    '2013_Egyptian_coup_d\\'%E9tat',\n",
      "    'Islamist_protests_in_Egypt_(July_2013%E2%80%93present)']}}]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def getPage(compiledURL,headerType,headerValue):\n",
      "    ''' This function calls a URL. It relies on a number of global variables being set: header1Type, header1Value, filesLocation'''\n",
      "    request = urllib2.Request(compiledURL)\n",
      "    request.add_header(headerType,headerValue)\n",
      "    i=0\n",
      "    while i<4:\n",
      "        try: \n",
      "            response = urllib2.urlopen(request)\n",
      "            headers = response.info()\n",
      "            data = response.read()\n",
      "            i=4\n",
      "        except urllib2.HTTPError, e:\n",
      "            fh=open(logsLocation+'RequestFailures.log','a')\n",
      "            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'HTTPError = ' + str(e.code))\n",
      "            fh.close()\n",
      "            i=i+1\n",
      "            time.sleep(2)\n",
      "            data=''\n",
      "            print 'HTTPError'\n",
      "        except urllib2.URLError, e:\n",
      "            fh=open(logsLocation+'RequestFailures.log','a')\n",
      "            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'URLError = ' + str(e.reason))\n",
      "            fh.close()\n",
      "            i=i+1\n",
      "            time.sleep(2)\n",
      "            data=''\n",
      "            print 'URLError'\n",
      "        except httplib.HTTPException, e:\n",
      "            fh=open(logsLocation+'RequestFailures.log','a')\n",
      "            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'HTTPException')\n",
      "            fh.close()\n",
      "            i=i+1\n",
      "            time.sleep(2)\n",
      "            data=''\n",
      "            print 'HTTPException'\n",
      "        except Exception:\n",
      "            import traceback\n",
      "            fh=open(logsLocation+'RequestFailures.log','a')\n",
      "            fh.write('\\n at '+str(datetime.now())+', From: '+compiledURL+'\\n'+'generic exception: ' + traceback.format_exc())\n",
      "            fh.close()\n",
      "            i=i+1\n",
      "            time.sleep(2)\n",
      "            data=''\n",
      "            print 'exception'\n",
      "    print \"called: \"+ compiledURL;\n",
      "    return data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def extractFootNotes(topicURL):\n",
      "    topicData=BeautifulSoup(getPage(topicURL,header1Type,header1Value),'html5lib')\n",
      "    \n",
      "    Footnotes=[]\n",
      "    citations=topicData.findAll('span',{'class': re.compile('^citation')})\n",
      "    # check for common (annoying) archiving sites:\n",
      "    #print citations\n",
      "    for citation in citations:\n",
      "        if len(citation.findAll('a',{'class': re.compile('^external')})) ==1:\n",
      "            Footnotes.append(citation.findAll('a',{'class':re.compile('^external')})[0])\n",
      "        else:\n",
      "            if len(citation.findAll('a',{'class': re.compile('^external')})) >1:\n",
      "                for link in citation.findAll('a',{'class': re.compile('^external')}):\n",
      "                    archiveService=['webcitation.org','web.archive.org','archive.is','dx.doi.org']\n",
      "                    href=str(link.get('href'))\n",
      "                    if any(x in href for x in archiveService):\n",
      "                        webcitationLog=open(logsLocation+'badcitationLog.log','a')\n",
      "                        problemcite=str(link.get('href'))+' triggered this citation condition from:'+str(citation)\n",
      "                        webcitationLog.write(topicURL+'\\n')\n",
      "                        webcitationLog.write(problemcite)\n",
      "                        webcitationLog.write('\\n\\n')\n",
      "                        webcitationLog.close()\n",
      "                       # print 'citation used an archive service'\n",
      "                    else:\n",
      "                        #print 'more than two links, we\\'re choosing: '+str(link)\n",
      "                        leftoverLog=open(logsLocation+'SelectionFromArchive.log','a')\n",
      "                        problemcite='Citation has more than two links, we\\'re including: '+str(link.get('href'))+'\\n from:'+str(citation)\n",
      "                        leftoverLog.write(topicURL+'\\n')\n",
      "                        leftoverLog.write(problemcite)\n",
      "                        leftoverLog.write('\\n\\n')\n",
      "                        leftoverLog.close()\n",
      "                        Footnotes.append(link)\n",
      "    #print Footnotes\n",
      "    #OK, we should have clean footnotes, now sort them into domains, count them etc.\n",
      "    URLDict={'ExternalURLs':[]}\n",
      "    for footnote in Footnotes:\n",
      "        URL=re.findall(r'href=[\\'\"]?([^\\'\" >]+)',str(footnote))\n",
      "        if URL[0].startswith(\"http:\") == True:\n",
      "            URLDict['ExternalURLs'].append(URL[0])\n",
      "    #make it easier to read\n",
      "    URLDict['ExternalURLs'].sort()\n",
      "    #count the number of ExternalURLS\n",
      "    URLLog=open(logsLocation+'URLLog.log','a')\n",
      "    URLLog.write('\\n\\n\\n\\n'+topicURL+' at:')\n",
      "    timestamp = str(datetime.now())\n",
      "    URLLog.write(timestamp+'\\n\\n')\n",
      "    URLDict['NumberOfURLs']=len(URLDict['ExternalURLs'])\n",
      "    # get the unique domains.\n",
      "    uniqueDomains={}\n",
      "    checkDomainCount=0\n",
      "    for URL in URLDict['ExternalURLs']:\n",
      "        URLLog.write(URL+'\\n')\n",
      "        domain =''\n",
      "        #checkeddomain =''\n",
      "        parsed_uri = urlparse(URL)\n",
      "        domain = '{uri.netloc}'.format(uri=parsed_uri).replace('www.','') #hey look, we're getting rid of the www. This may not be smart.\n",
      "        #start normalizing domains with multiple brands, could do by calling then scraping HTML for canonical URLs, but it's bandwidth & processor intensive, not OK for private budget.\n",
      "        #potentially contentious decision: businessweek being aggregated with bloomberg, Al Jazeeras America and English being aggregated, nbc + msn varients\n",
      "        \n",
      "        if domain=='':\n",
      "                print 'top: '+URL     \n",
      "        if domain[-12:] =='.abcnews.com' or re.search('abc(\\w*).go.com',domain):\n",
      "            rollLog=open(logsLocation+'abcnewsRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='abcnews.com'        \n",
      "        if 'http://www.google.com/hostednews/afp/' in str(URL):\n",
      "            rollLog=open(logsLocation+'afpRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='afp.com'\n",
      "        if domain in ['aljazeera.net'] or domain[-14:] =='.aljazeera.com':\n",
      "            rollLog=open(logsLocation+'alJazzRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='aljazeera.com'\n",
      "        if 'http://www.google.com/hostednews/ap/' in str(URL) or domain[-7:]=='.ap.org': #this captures some 'hosted' URLs\n",
      "            rollLog=open(logsLocation+'apRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='ap.org'\n",
      "        if domain[-17:] =='.baltimoresun.com':\n",
      "            rollLog=open(logsLocation+'baltimoreSunRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='baltimoresun.com'\n",
      "        if domain in ['bbc.co.uk','news.bbc.co.uk','bbcnews.co.uk','bbcnews.com']:\n",
      "            rollLog=open(logsLocation+'bbcRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='bbc.co.uk'\n",
      "        if domain in ['boston.com'] or domain[-16:] =='.bostonglobe.com':\n",
      "            rollLog=open(logsLocation+'bostonGlobeJazzRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='bostonglobe.com'\n",
      "        if domain[-16:] == 'businessweek.com':\n",
      "            rollLog=open(logsLocation+'BloombergRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='bloomberg.com'\n",
      "        if domain[-13:] =='.cbslocal.com': \n",
      "            rollLog=open(logsLocation+'cbsRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='cbsnews.com'\n",
      "        if domain[-8:]=='.cnn.com': \n",
      "            rollLog=open(logsLocation+'cnnRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='cnn.com'\n",
      "        if domain[-11:]=='.forbes.com':\n",
      "            rollLog=open(logsLocation+'forbesRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='forbes.com'\n",
      "        if domain[-7:]=='.ft.com':\n",
      "            rollLog=open(logsLocation+'ftRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='ft.com'\n",
      "        if domain in ['guardian.co.uk','theguardian.co.uk','theguardiannews.com','guardiannews.com','theguardian.com','m.guardiannews.com']:#deliberately verbose: there are copycat guardian brands.\n",
      "            rollLog=open(logsLocation+'guardianRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='theguardian.com'\n",
      "        if domain[-12:] == '.latimes.com':\n",
      "            rollLog=open(logsLocation+'latimesRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='latimes.com'\n",
      "        if domain in ['msnbc.msn.com','msnbcmedia.msn.com'] or domain[-12:] == '.nbcnews.com' or re.search('nbc(\\w*).com',domain): #too greedy?\n",
      "            rollLog=open(logsLocation+'nbcRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='nbcnews.com'\n",
      "        if domain[-12:]== '.nytimes.com':\n",
      "            rollLog=open(logsLocation+'nytimesRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='nytimes.com'\n",
      "        if domain[-8:] == '.npr.org': \n",
      "            rollLog=open(logsLocation+'nprRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='npr.org'\n",
      "        if domain[-12:]=='.reuters.com':\n",
      "            rollLog=open(logsLocation+'reutersRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='reuters.com'\n",
      "        if domain[-20:]=='.orlandosentinel.com':\n",
      "            rollLog=open(logsLocation+'orlandosentinelUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='orlandosentinel.com'\n",
      "        if domain[-16:]=='.telegraph.co.uk':\n",
      "            rollLog=open(logsLocation+'telegraphRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='telegraph.co.uk'\n",
      "        if domain[-13:]=='.usatoday.com':\n",
      "            rollLog=open(logsLocation+'usaTodayRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='usatoday.com'\n",
      "        if domain[-15:]=='.news.yahoo.com':\n",
      "            rollLog=open(logsLocation+'yahooRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='news.yahoo.com'\n",
      "        if domain[-19:]=='.washingtonpost.com':\n",
      "            rollLog=open(logsLocation+'washingtonpostRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='washingtonpost.com'\n",
      "        if domain[-8:]=='.wsj.com':\n",
      "            rollLog=open(logsLocation+'wsjRollUpLog.log','a')\n",
      "            rollLog.write(str(URL)+':\\n'+domain+'\\n\\n')\n",
      "            rollLog.close()\n",
      "            domain='wsj.com'\n",
      "        else:\n",
      "            checkeddomain=domain\n",
      "        if checkeddomain in uniqueDomains.keys():\n",
      "            uniqueDomains[checkeddomain]=uniqueDomains[checkeddomain]+1\n",
      "            URLLog.write('resolves to: '+checkeddomain+'\\n')\n",
      "        else:\n",
      "            uniqueDomains[checkeddomain]=1\n",
      "            URLLog.write('resolves to: '+checkeddomain+'\\n')\n",
      "    URLLog.close()\n",
      "    URLDict['uniqueDomains']=uniqueDomains\n",
      "    URLDict['NumberofUniqueDomains']=len(URLDict['uniqueDomains'])\n",
      "    for Udomain in uniqueDomains: \n",
      "        checkDomainCount=uniqueDomains[Udomain]+checkDomainCount\n",
      "    URLDict['CheckCount']=checkDomainCount\n",
      "    if URLDict['CheckCount']!=URLDict['NumberOfURLs']:\n",
      "        print 'Dropped Footnotes!: URLs Sorted By Domain=' +str(URLDict['CheckCount'])+' but Number of URLs (Raw)= '+str(URLDict['NumberOfURLs'])\n",
      "    return URLDict['uniqueDomains']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Given a Wikipedia URL, we can now get the footnotes in a dictionary\n",
      "\n",
      "def runIt(Stories):    \n",
      "    topStoryFootnotes=[]\n",
      "    for story in Stories: \n",
      "        key=story.keys()[0]\n",
      "        keyDict={key:{}}\n",
      "        for wikipage in story[key]['wikipages']:\n",
      "            #topStoryFootnotes[story]={'results'={key=wikipage}}\n",
      "\n",
      "            results = extractFootNotes('http://en.wikipedia.org/wiki/'+wikipage) #maybe change this to simply take and pass the full wikipedia URL\n",
      "            keyDict[key][wikipage]=results\n",
      "        topStoryFootnotes.append(keyDict)\n",
      "    return topStoryFootnotes\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def countOverAllDomains(dataSource):\n",
      "    topDomains={}\n",
      "    topDomainsByStory={}\n",
      "    storyTopDomainsSorted={}\n",
      "    for story in dataSource:\n",
      "        topDomainsByStory[story.keys()[0]]={}\n",
      "        for wikipage in story:\n",
      "            for domainList in story[wikipage].keys():\n",
      "                for domain in story[wikipage][domainList].items():\n",
      "                    if domain[0] not in topDomains.keys():\n",
      "                        topDomains[domain[0]]=domain[1]\n",
      "                        \n",
      "                    else:\n",
      "                        topDomains[domain[0]]=topDomains[domain[0]]+domain[1]\n",
      "                    #now make a dictionary for each story\n",
      "                    if domain[0] not in topDomainsByStory[story.keys()[0]].keys():\n",
      "                        topDomainsByStory[story.keys()[0]][domain[0]]=domain[1]\n",
      "                    else:\n",
      "                        topDomainsByStory[story.keys()[0]][domain[0]]=topDomainsByStory[story.keys()[0]][domain[0]]+domain[1]\n",
      "        storyTopDomainsSorted[story.keys()[0]]=sorted(topDomainsByStory[story.keys()[0]].items(), key=operator.itemgetter(1), reverse=True)\n",
      "        #storyDomains[story]=sorted(topDomainsByStory[story].items(), operator.itemgetter(1), reverse=True)\n",
      "    \n",
      "    sorted_Domains = sorted(topDomains.items(), key=operator.itemgetter(1), reverse=True) \n",
      "\n",
      "    return sorted_Domains, storyTopDomainsSorted\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smallData=runIt(storyList)\n",
      "rank=countOverAllDomains(smallData)\n",
      "\n",
      "#write out a Master List\n",
      "with open(outputLocation+'compiledResults.csv','w') as csvfile:\n",
      "    outputWriter=csv.writer(csvfile)\n",
      "    lst=['Domain','Citations']\n",
      "    outputWriter.writerow(lst) \n",
      "    for domain in rank[0]:\n",
      "        outputWriter.writerow(domain)\n",
      "csvfile.close\n",
      "\n",
      "#write out files for each story\n",
      "for story in rank[1].keys():\n",
      "    with open(outputLocation+story+'Results.csv','w') as csvSubFile:\n",
      "        outputWriter=csv.writer(csvSubFile)\n",
      "        lst=['Domain','Citations']\n",
      "        outputWriter.writerow(lst)  \n",
      "        for domain in rank[1][story]:            \n",
      "            outputWriter.writerow(domain)\n",
      "    csvfile.close\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}